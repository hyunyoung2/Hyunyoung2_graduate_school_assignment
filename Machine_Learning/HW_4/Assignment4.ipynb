{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fundamental of Deep Learning for Computer vision by NVIDIA](https://courses.nvidia.com/courses/course-v1:DLI+C-FX-01+V2/about) with [DIGIST of NVIDIA](https://github.com/NVIDIA/DIGITS)\n",
    "\n",
    "This is for me to rearrange the lecture above provided by nvidia to raise my skill about deep learning,\n",
    "\n",
    "This is summarized by me. \n",
    "\n",
    "The following is a image which is I am getting into lecture. \n",
    "\n",
    "![](picture/Fundamentals_of_Deep_learning_for_compute_vision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeplearning is inspired by an undestanding of humman learning. \n",
    "\n",
    "What is the difference between the classic matchine learning and Deep Learning?\n",
    "\n",
    "In the classic Machine learnig, Input is change to hand-designed features \n",
    "\n",
    "But, In the other case, it is ent-to-end learning which means you don't need to design the feature by hand from input. \n",
    "\n",
    "\n",
    "# Biological inspiration\n",
    "\n",
    "So Deeplearning cares about only input and output. the middle neuron is learned by a lot of emxamples.\n",
    "\n",
    "![](picture/Difference_in_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers and computing help us achieve more complex goals than we could do alone.\n",
    "\n",
    "However, conventially, computers could only follow the specific instructions they were given. \n",
    "\n",
    "When you solve problems with programming, you need to logically write instruction step-by-step for a compute  to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning allows computers to \"learn\" from examples(data).\n",
    "\n",
    "Solving problems with deep learning requires identifying some pattern in the world, finding exmaples that higlight bot sides of the pattern(the input and the output) and then letting a \"neural network\" learn the map between the two.\n",
    "\n",
    "So Deep learning learn the pattern from the examples by removing the need to write explicit instructions for your own problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tribe\n",
    "\n",
    "There are a lot of machine learning tribes as follows.\n",
    "\n",
    "![](picture/tribes.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks: GPU Task 1\n",
    "\n",
    "\n",
    "The task is called image classification where the image as input of model is \"Louie or not Louie\"\n",
    "\n",
    "after training, you can check your model's output like this:\n",
    "\n",
    "![](picture/rouie_1.png)\n",
    "\n",
    "\n",
    "Let's improve the model by training a few more times.\n",
    "\n",
    "\n",
    "Let's see the loss and learning rate in a few more times than the prior, 2 epoch. \n",
    "\n",
    "![](picture/epoch_100.png)\n",
    "\n",
    "the classified result with epoch 100\n",
    "\n",
    "\n",
    "![](picture/epoch_100_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data: GPU Task 2\n",
    "\n",
    "under this task, you will combine three ingredients below to train neural network \n",
    "\n",
    "1. Deep Neural network \n",
    "2. The GPU\n",
    "3. Big data\n",
    "\n",
    "Before entering the task, we need the large dataset. Fortunately, [kaggle](https://www.kaggle.com/datasets) has has a dataset that we can start with consisting of 18750 labeled images of dogs and cats.\n",
    "\n",
    "From now on, we teach our model what a dog is other than who Louie is. \n",
    "\n",
    "\n",
    "Let's see the resulting dataset in DIGITS \n",
    "\n",
    "\n",
    "The following is the dataset split into two sets for training and validation.\n",
    "\n",
    "The blue bar on the right side of histogram is dog images, and the other part is cat images.\n",
    "\n",
    "If you want to know the actual data, select the \"Explore the db\".\n",
    "\n",
    "![](picture/dogs_and_cats.png)\n",
    "\n",
    "\n",
    "You can train your model from train dataset, and then you infer the output form unseen data to classify the input making desicion based on what was learned called inference.\n",
    "\n",
    "Les's see the process of training our model which is \"Alexnet\"\n",
    "\n",
    "![](picture/AlexNet_training.png)\n",
    "\n",
    "I will test my model with the unseen data, \"louiestest2.JPG\". the following is the inference of my model. \n",
    "\n",
    "![](picture/inference1.png)\n",
    "\n",
    "\n",
    "after seeing the result, you can interpret it in two ways. \n",
    "\n",
    "```\n",
    "1) It worked! We took an untrained neural network, exposed it to thousands of *labeled* images, and it outputs a statistically significant confidence that a dog should be classified as a dog. Congratulations!  \n",
    "\n",
    "2) We're not there yet. Human learners would be 100% confident that that image contained a dog. Our model still has significant loss.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deploy the model \n",
    "\n",
    "\n",
    "The deep learning workflow has two distinct sections: training and deployment.\n",
    "\n",
    "Deployment is the work of taking a trained model and putting it ot work as a part of an application. You can deploy to edge devices such as robots or autonomous vehicels. you can alos deploy to a server in order to play a role in any piece of software.\n",
    "\n",
    "![](picture/deployment1.png)\n",
    "\n",
    "\n",
    "Typcially the deep learning model is complex, however if you know the input and output on deep learning model. \n",
    "\n",
    "You could deal with the model as a function to generate output regarding input. \n",
    "\n",
    "![](picture/function1.png)\n",
    "\n",
    "To successfully deploy a trained model, we have two jobs.\n",
    "\n",
    "```\n",
    "1) Our first job is to provide our model an input that it expects.  \n",
    "\n",
    "2) Our second job is to provide our end user an output that is useful.  \n",
    "```\n",
    "\n",
    "the input that our model expects is decided by model architecture and how it was trained. \n",
    "\n",
    "So you have to write code in the front of the model to convert the input you have to the input the model expects. \n",
    "\n",
    "Also the output is decided by model architecture and what it is learned. So it is the same from dealing with input, the output is converted to the output end user expect from the output generated from model.\n",
    "\n",
    "![](picture/the_component_of_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying our Model: GPU Task 3\n",
    "\n",
    "\n",
    "selct the model below\n",
    "\n",
    "![](picture/DGIST_for_Deploying.png)\n",
    "\n",
    "\n",
    "after entering the model, type in instructions below\n",
    "\n",
    "\n",
    "```\n",
    "MODEL_JOB_DIR = '##FIXME##'  ## Remember to set this to be the job directory for your model\n",
    "!ls $MODEL_JOB_DIR\n",
    "```\n",
    "\n",
    "Since I use the caffe for the model, my model consists of two files: the architecture and the weights.\n",
    "\n",
    "The architecture : model_name.prototxt\n",
    "The weight : snapshot_iter_#.caffemodel.\n",
    "\n",
    "```\n",
    "ARCHITECTURE = MODEL_JOB_DIR + '/' + 'deploy.prototxt'\n",
    "WEIGHTS = MODEL_JOB_DIR + '/' + 'snapshot_iter_735.caffemodel'\n",
    "print (\"Filepath to Architecture = \" + ARCHITECTURE)\n",
    "print(\"Filepath to weights = \"+ WEIGHTS)\n",
    "```\n",
    "\n",
    "let's make net which classify object as follows.\n",
    "\n",
    "```\n",
    "import caffe\n",
    "caffe.set_mode_gpu()\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(ARCHITECTURE, WEIGHTS,  \n",
    "                       channel_swap =(2, 1, 0), #Color images have three channels, Red, Green, and Blue.\n",
    "                       raw_scale=255) #Each pixel value is a number between 0 and 255\n",
    "                       #Each \"channel\" of our images are 256 x 256 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Expected Input: Preprocessing\n",
    "\n",
    "\n",
    "Let's see the preprocessing for my model \n",
    "\n",
    "The following is no-preprocessing. \n",
    "\n",
    "![](picture/Before.png)\n",
    "\n",
    "The follwoing is done with preprocessing for Alexnet.\n",
    "\n",
    "![](picture/after.png)\n",
    "\n",
    "\n",
    "The following is another preprocessing called \"normalize\"\n",
    "\n",
    "\n",
    "so far, input is dealt with as input my model expects.\n",
    "\n",
    "\n",
    "Finally, if you want vector probability, use the function below\n",
    "\n",
    "```\n",
    "# make prediction\n",
    "prediction = net.predict([ready_image])\n",
    "print prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a useful output: Postprocessing\n",
    "\n",
    "\n",
    "![](picture/output_1.png)\n",
    "\n",
    "\n",
    "Let's test other example for the doggy door.\n",
    "\n",
    "![](picture/postprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "![](picture/putting_it_all_together.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance during Training: GPU Task 4\n",
    "\n",
    "\n",
    "let's work through a full deep learning workflow:\n",
    "\n",
    "1) we prepare a dataset for training\n",
    "2) we select a network to train\n",
    "3) we train the network\n",
    "4) we test the trained model in a training environment\n",
    "5) we deploy the trained model into an application\n",
    "\n",
    "But, You don't need to have a trained model from scratch to deploy it.\n",
    "\n",
    "In this next section, we will engage with some strategies for improving training performance using our existing model. You will learn:\n",
    "\n",
    "- To run more training epochs on an existing model, analogous to a human learner studying more.\n",
    "- To search the hyperparameter space, analogous to a human learner responding differently to a different teaching style.\n",
    "- To use the results of others' research, compute, network design, and data, analogous to a human learner copying off an expert\n",
    "\n",
    "\n",
    "explore the dataset we will be using in this section, [imagenet](http://image-net.org/explore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate\n",
    "\n",
    "You may be wondering three things:\n",
    "\n",
    "```\n",
    "1) What is \"learning rate?\"\n",
    "2) Why does the learning rate decrease throughout the training session?\n",
    "3) Who controls that?\n",
    "\n",
    "1) Learning rate is the rate at which each \"weight\" changes during training. Each weight is moving in the direction that reduces loss at a value multiplied by the learning rate.\n",
    "2) The learning rate decreases throughout the training session because the network is getting closer to its ideal solution. At first, the network being trained knows nothing about what it may see. Nevermind dogs, it doesn't know whether it's about to see images or sensor data, etc. Large jumps towards the ideal solution make sense. After a few epochs, the weights are getting better so it's important for the network to be less reactive to each image it sees.\n",
    "3) You control the learning rate. The learning rate is one of many \"hyperparameters\" that we set when setting up the training session. We'll make an adjustment to it in just a minute. Here's why.\n",
    "\n",
    "Since at the end of the session the learning rate was slow, when starting from a pretrained network, we should pick up where we left off. Let's set up a training job from this pretrained model starting at the learning rate of .0001.\n",
    "```\n",
    "\n",
    "![](picture/learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see pretrained model. \n",
    "\n",
    "```\n",
    "Note the following:\n",
    "\n",
    "As expected, the accuracy starts close to where our first model left off, 80%.\n",
    "Accuracy DOES continue to increase, showing that increasing the number of epochs often does increase performance.\n",
    "The rate of increase in accuracy slows down, showing that more trips through the same data can't be the only way to increase performance.\n",
    "There are four categories of levers that you can manipulate to improve performance. Time spent learning about each of them will pay off in the performance of your models.\n",
    "\n",
    "1) Data - A large and diverse enough dataset to represent the environment where our model should work. Data curation is an art form in itself.\n",
    "2) Hyperparameters - Making changes to options like learning rate are like changing your training \"style.\" Currently, finding the right hyperparameters is a manual process learned through experimentation. As you build intuition about what types of jobs respond well to what hyperparameters, your performance will increase.\n",
    "3) Training time - More epochs improve performance to a point. At some point, too much training will result in overfitting (humans are guilty of this too), so this can not be the only intervention you apply.\n",
    "4) Network architecture - We'll begin to experiment with network architecture in the next section. This is listed as the last intervention to push back against a false myth that to engage in solving problems with deep learning, people need mastery of network architecture. This field is fascinating and powerful, and improving your skills is a study in math.\n",
    "```\n",
    "\n",
    "![](picture/pretraining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pretrained AlexNet, so type in the following commands\n",
    "\n",
    "```\n",
    "!wget http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel\n",
    "!wget https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/deploy.prototxt\n",
    "```\n",
    "\n",
    "Let's download the mean image from DIGITS\n",
    "\n",
    "```\n",
    "!wget https://github.com/BVLC/caffe/blob/master/python/caffe/imagenet/ilsvrc_2012_mean.npy?raw=true\n",
    "!mv ilsvrc_2012_mean.npy?raw=true ilsvrc_2012_mean.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the model with [reference](http://caffe.berkeleyvision.org/gathered/examples/imagenet.html) for preprocessing of ImageNet\n",
    "\n",
    "```\n",
    "import caffe\n",
    "import numpy as np\n",
    "caffe.set_mode_gpu()\n",
    "import matplotlib.pyplot as plt #matplotlib.pyplot allows us to visualize results\n",
    "\n",
    "ARCHITECTURE = 'deploy.prototxt'\n",
    "WEIGHTS = 'bvlc_alexnet.caffemodel'\n",
    "MEAN_IMAGE = 'ilsvrc_2012_mean.npy'\n",
    "TEST_IMAGE = '/filepath/louietest2.JPG'\n",
    "\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(ARCHITECTURE, WEIGHTS) #Each \"channe\n",
    "```\n",
    "\n",
    "Let's make input that the model expects\n",
    "\n",
    "```\n",
    "#Load the image\n",
    "image= caffe.io.load_image(TEST_IMAGE)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "#Load the mean image\n",
    "mean_image = np.load(MEAN_IMAGE)\n",
    "mu = mean_image.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "# set the size of the input (we can skip this if we're happy with the default; we can also change it later, e.g., for different batch sizes)\n",
    "net.blobs['data'].reshape(1,        # batch size\n",
    "                          3,         # 3-channel (BGR) images\n",
    "                          227, 227)  # image size is 227x227\n",
    "\n",
    "transformed_image = transformer.preprocess('data', image)\n",
    "```\n",
    "\n",
    "let's run the function,\n",
    "\n",
    "```\n",
    "# copy the image data into the memory allocated for the net\n",
    "net.blobs['data'].data[...] = transformed_image\n",
    "\n",
    "### perform classification\n",
    "output = net.forward()\n",
    "\n",
    "#output\n",
    "```\n",
    "\n",
    "let's make output that end user expects\n",
    "\n",
    "```\n",
    "output_prob = output['prob'][0]  # the output probability vector for the first image in the batch\n",
    "print 'predicted class is:', output_prob.argmax()\n",
    "```\n",
    "\n",
    "If you want to take a look at imagenet's classes to see what that number corresponds to the result, \n",
    "\n",
    "visit [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).\n",
    "\n",
    "Again using wget to get a dictionary (dict) of class to label.\n",
    "\n",
    "```\n",
    "!wget https://raw.githubusercontent.com/HoldenCaulfieldRye/caffe/master/data/ilsvrc12/synset_words.txt\n",
    "labels_file = 'synset_words.txt'\n",
    "labels = np.loadtxt(labels_file, str, delimiter='\\t')\n",
    "\n",
    "print 'output label:', labels[output_prob.argmax()]\n",
    "```\n",
    "\n",
    "The result of application \n",
    "\n",
    "```\n",
    "print (\"Input image:\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "print(\"Output label:\" + labels[output_prob.argmax()])\n",
    "```\n",
    "\n",
    "![](picture/output_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection: GPU Task 5\n",
    "\n",
    "\n",
    "With image classification, our network took an image and generated a classification. More specifically, our input was a 256X256X3 tensor of pixel values and our output was a 2-unit vector (since we had 2 classes) of probabilities.\n",
    "\n",
    "![](picture/classification.png)\n",
    "\n",
    "\n",
    "Before seeing the obeject detection task, let's see input-output mapping in several tasks A vector where each index corresponds with the likelihood or the image of belonging to each classg.\n",
    "\n",
    "| Workflow | input | Output |\n",
    "| --- | --- | --- |\n",
    "|Image Classification|Raw Pixel Values|A vector where each index corresponds with the likelihood or the image of belonging to each class|\n",
    "|Object Detection|Raw Pixel Values|A vector with (X,Y) pairings for the top-left and bottom-right corner of each object present in the image|\n",
    "|Image Segmentation|Raw Pixel Values|A overlay of the image for each class being segmented, where each value is the likelihood of that pixel belonging to each class|\n",
    "|Text Generation|A unique vector for each 'token' (word, letter, etc.)|A vector representing the most likely next 'token'|\n",
    "|Image Rendering|Raw Pixel Values of a grainy Image|Raw pixel values of a clean image|\n",
    "\n",
    "\n",
    "let's see computer vision tasks \n",
    "\n",
    "![](picture/vision_tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new task is dectecting and localize objects within images.\n",
    "\n",
    "```\n",
    "import time\n",
    "import numpy as np #Data is often stored as \"Numpy Arrays\"\n",
    "import matplotlib.pyplot as plt #matplotlib.pyplot allows us to visualize results\n",
    "import caffe #caffe is our deep learning framework, we'll learn a lot more about this later in this task.\n",
    "%matplotlib inline\n",
    "\n",
    "MODEL_JOB_DIR = 'path/'  ## Remember to set this to be the job directory for your model\n",
    "DATASET_JOB_DIR = 'path/'  ## Remember to set this to be the job directory for your dataset\n",
    "\n",
    "MODEL_FILE = MODEL_JOB_DIR + '/deploy.prototxt'                 # This file contains the description of the network architecture\n",
    "PRETRAINED = MODEL_JOB_DIR + '/snapshot_iter_735.caffemodel'    # This file contains the *weights* that were \"learned\" during training\n",
    "MEAN_IMAGE = DATASET_JOB_DIR + '/mean.jpg'                      # This file contains the mean image of the entire dataset. Used to preprocess the data.\n",
    "\n",
    "# Tell Caffe to use the GPU so it can take advantage of parallel processing. \n",
    "# If you have a few hours, you're welcome to change gpu to cpu and see how much time it takes to deploy models in series. \n",
    "caffe.set_mode_gpu()\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))\n",
    "\n",
    "# load the mean image from the file\n",
    "mean_image = caffe.io.load_image(MEAN_IMAGE)\n",
    "print(\"Ready to predict.\")\n",
    "```\n",
    "\n",
    "assume that we get image from the security camera, but it is larger than input of model. \n",
    "\n",
    "![](picture/second_for_detecting.png)\n",
    "\n",
    "Let's see forward propagation \n",
    "\n",
    "![](picture/third_for_detecting.png)\n",
    "\n",
    "The output above is the output of the last layer of the network we're using. The type of layer is \"softmax\"\n",
    "\n",
    "\n",
    "Let's see another code to detect object \n",
    "\n",
    "```\n",
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many 256x256 grid squares are in the image\n",
    "rows = input_image.shape[0]/256\n",
    "cols = input_image.shape[1]/256\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "\n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # subtract the mean image\n",
    "        grid_square -= mean_image\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square]) \n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections, interpolation=None)\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time: ' + str(end-start) + ' seconds'\n",
    "```\n",
    "\n",
    "![](picture/Forth_for_detecting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With another solution, how to use an image classification to detect objects \n",
    "\n",
    "```\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import time\n",
    "\n",
    "MODEL_JOB_DIR = '##FIXME##'  ## Remember to set this to be the job number for your model\n",
    "DATASET_JOB_DIR = '##FIXME##'  ## Remember to set this to be the job number for your dataset\n",
    "\n",
    "MODEL_FILE = MODEL_JOB_DIR + '/deploy.prototxt'                 # Do not change\n",
    "PRETRAINED = MODEL_JOB_DIR + '/snapshot_iter_735.caffemodel'    # Do not change\n",
    "MEAN_IMAGE = DATASET_JOB_DIR + '/mean.jpg'                      # Do not change\n",
    "\n",
    "# load the mean image\n",
    "mean_image = caffe.io.load_image(MEAN_IMAGE)\n",
    "\n",
    "# Choose a random image to test against\n",
    "#RANDOM_IMAGE = str(np.random.randint(10))\n",
    "IMAGE_FILE = '/dli/data/LouieReady.png' \n",
    "\n",
    "# Tell Caffe to use the GPU\n",
    "caffe.set_mode_gpu()\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))\n",
    "\n",
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many 256x256 grid squares are in the image\n",
    "rows = input_image.shape[0]/256\n",
    "cols = input_image.shape[1]/256\n",
    "\n",
    "# Subtract the mean image\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        input_image[i*256:(i+1)*256,j*256:(j+1)*256] -= mean_image\n",
    "        \n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "        \n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square])\n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time (sliding window without overlap): ' + str(end-start) + ' seconds'\n",
    "\n",
    "# define the amount of overlap between grid cells\n",
    "OVERLAP = 0.25\n",
    "grid_rows = int((rows-1)/(1-OVERLAP))+1\n",
    "grid_cols = int((cols-1)/(1-OVERLAP))+1\n",
    "\n",
    "print \"Image has %d*%d blocks of 256 pixels\" % (rows, cols)\n",
    "print \"With overlap=%f grid_size=%d*%d\" % (OVERLAP, grid_rows, grid_cols)\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((grid_rows,grid_cols))\n",
    "\n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,grid_rows):\n",
    "    for j in range(0,grid_cols):\n",
    "        start_col = int(j*256*(1-OVERLAP))\n",
    "        start_row = int(i*256*(1-OVERLAP))\n",
    "        grid_square = input_image[start_row:start_row+256, start_col:start_col+256]\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square])\n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print ('Total inference time (sliding window with %f%% overlap: ' % (OVERLAP*100)) + str(end-start) + ' seconds'\n",
    "\n",
    "# now with batched inference (one column at a time)\n",
    "# we are not using a caffe.Classifier here so we need to do the pre-processing\n",
    "# manually. The model was trained on random crops (256*256->227*227) so we\n",
    "# need to do the cropping below. Similarly, we need to convert images\n",
    "# from Numpy's Height*Width*Channel (HWC) format to Channel*Height*Width (CHW) \n",
    "# Lastly, we need to swap channels from RGB to BGR\n",
    "net = caffe.Net(MODEL_FILE, PRETRAINED, caffe.TEST)\n",
    "start = time.time()\n",
    "net.blobs['data'].reshape(*[grid_cols, 3, 227, 227])\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # add to batch\n",
    "        grid_square = grid_square[14:241,14:241] # 227*227 center crop        \n",
    "        image = np.copy(grid_square.transpose(2,0,1)) # transpose from HWC to CHW\n",
    "        image = image * 255 # rescale\n",
    "        image = image[(2,1,0), :, :] # swap channels\n",
    "        net.blobs['data'].data[j] = image\n",
    "    # make prediction\n",
    "    output = net.forward()[net.outputs[-1]]\n",
    "    for j in range(0,cols):\n",
    "        detections[i,j] = output[j].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time (batched inference): ' + str(end-start) + ' seconds'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fully Convolutional network, \n",
    "\n",
    "\n",
    "```\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import copy\n",
    "from scipy.misc import imresize\n",
    "import time\n",
    "\n",
    "MODEL_FILE = JOB_DIR + '/deploy.prototxt'                 # Do not change\n",
    "PRETRAINED = JOB_DIR + '/snapshot_iter_735.caffemodel'    # Do not change                 \n",
    "\n",
    "# Tell Caffe to use the GPU\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "# This time the model input size is reshaped based on the randomly selected input image\n",
    "net = caffe.Net(MODEL_FILE,PRETRAINED,caffe.TEST)\n",
    "net.blobs['data'].reshape(1, 3, input_image.shape[0], input_image.shape[1])\n",
    "net.reshape()\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "transformer.set_channel_swap('data', (2,1,0))\n",
    "transformer.set_raw_scale('data', 255.0)\n",
    "\n",
    "# This is just a colormap for displaying the results\n",
    "my_cmap = copy.copy(plt.cm.get_cmap('jet')) # get a copy of the jet color map\n",
    "my_cmap.set_bad(alpha=0) # set how the colormap handles 'bad' values\n",
    "\n",
    "# Feed the whole input image into the model for classification\n",
    "start = time.time()\n",
    "out = net.forward(data=np.asarray([transformer.preprocess('data', input_image)]))\n",
    "end = time.time()\n",
    "\n",
    "# Create an overlay visualization of the classification result\n",
    "im = transformer.deprocess('data', net.blobs['data'].data[0])\n",
    "classifications = out['softmax'][0]\n",
    "classifications = imresize(classifications.argmax(axis=0),input_image.shape,interp='bilinear').astype('float')\n",
    "classifications[classifications==0] = np.nan\n",
    "plt.imshow(im)\n",
    "plt.imshow(classifications,alpha=.5,cmap=my_cmap)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time: ' + str(end-start) + ' seconds'\n",
    "```\n",
    "\n",
    "![](picture/fifth_for_detecting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DetectNet which an Fully Convolutional Networkd, The bulk of the layers in DetectNet are identical to the well-known GoogLeNet network.\n",
    "\n",
    "Let's see the type of data, input and output for dectecting \n",
    "\n",
    "![](picture/sixth_for_detecting.png)\n",
    "\n",
    "Note:\n",
    "\n",
    "1) The input and its corresponding output are correlated based on the file number.\n",
    "2) The vector you're seeing consists of the (x,y) coordinates of the top left and bottom corner of the dog in the input image.\n",
    "3) If we had enough data with surfboards, we could train a surfboard detector instead of a dog detector! We'll set our dataset to only look for dogs.\n",
    "\n",
    "\n",
    "The following is the result from DetectNet.\n",
    "\n",
    "![](picture/detect_net.png)\n",
    "\n",
    "\n",
    "The takeaway is \n",
    "\n",
    "```\n",
    "1. The right network and data for the job at hand are vastly superior to hacking your own solution.\n",
    "2. The right network (and sometimes even pretrained model) might be available in DIGITS or on the [Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo) of your framework.\n",
    "\n",
    "\n",
    "So one problem solving framework you can use is:\n",
    "\n",
    "- determine if someone else has already solved your problem, use their model\n",
    "- if they have not, determine if someone else has already solved a problem like yours, use their network and your data\n",
    "- if they have not, determine if you can identify the shortcomings of someone else's solution in solving your problem and work to design a new network\n",
    "- if you can not, use an existing solution other problem solving techniques (eg. python) to solve your problem\n",
    "- either way, continue to experiment and take labs to increase your skill set!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    "\n",
    "- [ImageNet](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/)\n",
    "\n",
    "- [Use the NVIDIA AMI on AWS ( 10 minutes)](https://github.com/NVIDIA/nvidia-docker/wiki/Deploy-on-Amazon-EC2)\n",
    "\n",
    "- [Get started with nvidia-docker (5 minutes)](https://github.com/NVIDIA/nvidia-docker)\n",
    "\n",
    "- [Get started with the NVIDIA DIGITS container ( 5 minutes)](https://github.com/NVIDIA/nvidia-docker/wiki/DIGITS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
